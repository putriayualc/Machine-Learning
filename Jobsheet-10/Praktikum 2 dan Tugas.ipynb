{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyO1VYv9OyGw5I2sgS8FA4jt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/putriayualc/Machine-Learning/blob/main/Jobsheet-10/Praktikum%202%20dan%20Tugas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nama : Putri Ayu Aliciawati  \n",
        "Kelas : TI-3C  \n",
        "Absen : 21  \n",
        "NIM : 2241720132"
      ],
      "metadata": {
        "id": "i_08BsEMTWxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Praktikum 2"
      ],
      "metadata": {
        "id": "5JIaNwELTNrR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "WbIOmB9gThtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import TensorFlow"
      ],
      "metadata": {
        "id": "CoWvlmS1Tja7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "fsLd-GsKThPs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Dataset Shakespeare"
      ],
      "metadata": {
        "id": "O9P_MM6nTuvv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UckzUUcXTHhy",
        "outputId": "a60f33b5-5b07-4bd0-8f7c-4e079f9d0002"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "48lF9KgpT2xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCvANifPT4qu",
        "outputId": "4756e288-5d4f-4d1e-c7ca-a0379e266fa2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVm46NXgT635",
        "outputId": "c307808c-9ed1-4916-fde5-6b08a9968f2d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcrahcuzT8Zu",
        "outputId": "ef55be42-7b43-49dc-a34b-04a11bf39e12"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Olah Teks"
      ],
      "metadata": {
        "id": "aVocZRGKT91J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorize Teks"
      ],
      "metadata": {
        "id": "KEDjwGJ6T_A1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PsLEoFVUCyY",
        "outputId": "60d337a7-9763-4a34-b5d6-2b5044b447c0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "GJb3_UxeUKaG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-cku4ZIUQXw",
        "outputId": "dc5ec0ae-7fd7-4a6e-d109-93de332212af"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "4Yz2s4PFUTS9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tVGhyvOUVjc",
        "outputId": "b83b98a4-da72-4f6a-d4c0-04e0a30c1644"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTzXGNKOUYfP",
        "outputId": "dcc43d06-890b-4699-a545-14c26b88a210"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "9G91-_rcUai3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediksi"
      ],
      "metadata": {
        "id": "0xHwF2rNUfd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3yLIXUVUgm3",
        "outputId": "b0d2a4de-4c99-4a79-e058-70cd30009d5b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "D-tEkReeUliz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwH3TdGJUoFm",
        "outputId": "155343d5-242b-4159-eac1-d56cf3061f44"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "IvxcSMowUrs9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUgfDLXsUwIp",
        "outputId": "df1d6e04-9187-4507-8007-a32f25503b77"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AdTBvgGUy9_",
        "outputId": "f212b864-83b8-4f97-fe04-1923f566b8d4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text"
      ],
      "metadata": {
        "id": "cLfg5V9aU29u"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Zw2VZ4NVCem",
        "outputId": "904574a9-ffff-4c46-cbea-63f5c294f064"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "65u71_MdVEJY"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "  print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "  print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpGwZb0KVHJo",
        "outputId": "3fb4e2d1-5246-4a15-9fa8-b669e90c5a4b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Membuat Batch Training"
      ],
      "metadata": {
        "id": "R44hm7HjVOqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gv23RKCYVQhR",
        "outputId": "78a8f9c4-3953-4131-8dee-8a161937ccac"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Buat Model"
      ],
      "metadata": {
        "id": "VKbui9lCVTs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "1bIBW8JsVUw3"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "uMk52TTgVWZ9"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "KFf3F_rQVYM3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uji Model"
      ],
      "metadata": {
        "id": "nWKL5tIYVakp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNlQKVc1Vb0u",
        "outputId": "efc99ddd-e7fa-4224-bbf6-8b98a24f7354"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3VPMkijVf0s",
        "outputId": "6859d9de-9ebc-4d53-e073-80f5b10d5752"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "F0tQDk_cVhq5"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EMZx7KmVpIV",
        "outputId": "e1706bb8-1c7d-421a-a708-8ee84d03b2f4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([24, 16, 21, 31, 13, 49, 10, 18, 31,  2,  6, 33, 22, 48, 17,  4, 20,\n",
              "       34, 59, 49, 55, 35, 43, 37, 25,  8, 64, 58, 64, 56,  7, 21, 19, 18,\n",
              "       16,  0, 19, 23, 59,  0, 18, 52, 11, 63, 13, 18, 41, 10, 34, 41, 56,\n",
              "       33, 41, 23, 14, 59, 24, 44, 52, 61, 15,  9, 50, 54, 58, 44,  3, 30,\n",
              "        9, 21, 57, 23, 53, 37, 24, 30, 64,  7, 40, 57, 26,  8, 64, 20, 36,\n",
              "       43, 60, 65, 12, 46, 13, 38, 19, 10,  5,  4, 32, 64, 41, 11])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pWUwo6WVqkh",
        "outputId": "dff9a168-f482-4719-91bb-d392a16f3dce"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\"me one of you would fly from us,\\nThat there's no hoped-for mercy with the brothers\\nMore than with ru\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"KCHR?j3ER 'TIiD$GUtjpVdXL-ysyq,HFEC[UNK]FJt[UNK]Em:x?Eb3UbqTbJAtKemvB.kose!Q.HrJnXKQy,arM-yGWduz;g?YF3&$Syb:\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model\n",
        "### Tambahan optimizer dan fungsi loss"
      ],
      "metadata": {
        "id": "ojLPMDCAVv31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "m-zmFAxsVtv2"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBIuiQBhV2ue",
        "outputId": "d7f11068-8bd7-458e-a5b6-20b75d3cfd95"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1890025, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVvtnuQ_V5Yh",
        "outputId": "1597d8e3-954e-4f97-e0df-38541cfbb9bc"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.95697"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "EbLYhjxvV6sP"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Konfigurasi Checkpoints"
      ],
      "metadata": {
        "id": "Re5G6gBEV_p3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "HSd82WQPV7c7"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lakukan Proses Training"
      ],
      "metadata": {
        "id": "-PNNt9EeWD_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "-NqHzA94WGDm"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jDhMfD6WJvC",
        "outputId": "955c1890-28b1-4ef7-bd35-54e3eeaf239f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 124s 705ms/step - loss: 2.7326\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 122s 703ms/step - loss: 1.9943\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 118s 683ms/step - loss: 1.7108\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 125s 721ms/step - loss: 1.5480\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 123s 707ms/step - loss: 1.4493\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 117s 670ms/step - loss: 1.3824\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 117s 676ms/step - loss: 1.3301\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 118s 681ms/step - loss: 1.2859\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 120s 691ms/step - loss: 1.2446\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 118s 679ms/step - loss: 1.2064\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 119s 685ms/step - loss: 1.1664\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 116s 671ms/step - loss: 1.1257\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 116s 670ms/step - loss: 1.0825\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 117s 674ms/step - loss: 1.0364\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 117s 673ms/step - loss: 0.9892\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 118s 680ms/step - loss: 0.9387\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 119s 688ms/step - loss: 0.8870\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 119s 688ms/step - loss: 0.8359\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 121s 698ms/step - loss: 0.7843\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 122s 703ms/step - loss: 0.7352\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Teks"
      ],
      "metadata": {
        "id": "8VG4smHiWSbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "dBbRpWM-WXKk"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "dln2k2UeWZMH"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xodUoiQLWdD8",
        "outputId": "00332729-56bf-45f4-dc5a-f96da95cc30a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "That when the sea?\n",
            "\n",
            "ESCALUS:\n",
            "The sought this man's woe, thou liest.\n",
            "Thy lord again, or to fight with mildness.\n",
            "\n",
            "PETRUCHIO:\n",
            "To thee a king, without children; only to subject.\n",
            "\n",
            "LUCENTIO:\n",
            "Alack, for France, have ta'en the sea.\n",
            "\n",
            "BRAKENBURY:\n",
            "I knock him in his lady as my wife:\n",
            "Since we are thou, tender-shriedding flowers of my own distress.\n",
            "\n",
            "BRUTUS:\n",
            "Come, shall I have.\n",
            "\n",
            "BENVOLIO:\n",
            "That's such a heavy moon, were ride's subst;\n",
            "And who should say, 'Lover i' the right of your age:\n",
            "I would they were to murder: mysteries\n",
            "That thou art not a lady:' doth shew myself,\n",
            "Take the owe unwillingness Liur dather.\n",
            "And never had broke--the boy;\n",
            "Of our cowards their own youth, if my referbrawn\n",
            "your loves chastises, that was my wife.\n",
            "\n",
            "Lord:\n",
            "His name is Kater? 'Tis pardon 'em, deportuner;\n",
            "Sometime she demissed upon him.\n",
            "\n",
            "BUCKINGHAM:\n",
            "No, sir, is it not kiss the people; for how mayst thou\n",
            "love a woman:\n",
            "Nuely to perform it from my head;\n",
            "Young English marriage, we accomp out.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Where is my son, youn \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.56242299079895\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkhhTyG3WemJ",
        "outputId": "8421bc5e-59e7-41bf-8289-251b5baeea5b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nThe lords of Rome; no doubt, son\\nWas 'dward's; be made a goodly presence\\nThan we finde, butt with a goodly base:\\nProud-queen of patience to e'er come to me;\\nUnder our that way thou art not hand of heaven;\\nCould not over-me impartial; believe geath.\\n\\nVOLUMNIA:\\nThe children yours upon him what let her intermand\\nThat thou wert round'st to tread-she thunder?\\n\\nLUCIO:\\nI cannot tell to this.\\nFor live, I do, I wish't.\\nMy love, our fourth, every trial of our highness\\nTo be entirely death. O Ghoutster's, wife?\\n\\nPARIS:\\nI got that in his grave speak; let me live,\\nTo be your time to her! You'r boundy slew is a\\ngreat palace bowlesh of his neck.\\n\\nANThIONE:\\nGo to, against From From him\\nDid this day read for and cureden: for, how age!\\nWhat I was great, and more of this directing:\\nit is much guilty oness, that hour I run?\\nYou have pun her maid at once than death without thy frot?\\n\\nESCALUS:\\nWhy, lady, as you were king?\\n\\nJULIET:\\nWhy, how now, thou art welcome, do must be thunder?\\n\\nNurse:\\nJup there a move\"\n",
            " b\"ROMEO:\\nCome, sir, the case is gone; and that's my friends; I have no\\nnew-known, and where once again. This can some pleasure.\\n\\nPETRUCHIO:\\nYou have, from Margail.\\n\\nFRIAR LAURENCE:\\nAh, Juliet, so hith you to visit them.\\n\\nBUCKINGHAM:\\nWelcome, sweet Warwick, Citizen:\\nCome, thou wilt will, and with the words of joyalth of rog\\nbetter.\\n\\nAll:\\nPoor Clifford!\\n\\nSICINIUS:\\nThis must my easie go\\nYou thus but virties. 'Zounds, and to petite it.\\n\\nELBOW:\\nMarry, pilgrim, you quake of you.\\n\\nCOMINIUS:\\nI do conce more straight\\nbe a parle; though that brought them morth and knock;\\nWhich she--a resser of mine,\\nChilford live, will soon our suns. Thus 'tis\\ncurrent for this pretition. Yet, if he\\nrefused dishonour to importune,\\nWhich accompany is ready? and why not in he\\nvilled away; and, as they were a horry bless\\nThe shepherds. We do delay\\nWe are the sir. Your brother's son.\\n\\nGLOUCESTER:\\nMy lord?\\n\\nKING RICHARD III:\\nThe tenant to this brother Edward!\\nWe must be the begming mightires of his fire,\\nFor that I have no ot\"\n",
            " b\"ROMEO:\\nShall this news is,\\nBecause that thy become to flad thee,\\nHaving no more than his offence.\\n\\nTHOMAS MOWWBRAY:\\nThe young conferent were I might to visit\\nThe entertainme is ward and take thee in his honour.\\nEre you were to dear, how seems you from desirn\\nAnd last the Baptista side, I pine himself.\\n\\nCORIOLANUS:\\n'Zounds, good madam:\\nTo save your Capil's in Coriolities,\\nOr free the wardants sparels to the crown.\\nDid this break-hearted most sweet present,\\nYour dather lies, for he is valued throne,\\nTo whom the strong statue is Edward's face;\\nHappier neath, officer, we beseech you;\\nHave you too entering the oath that I groan so,\\nThat I may sour mise overthrow Morcius.\\n\\nSICINIUS:\\n'Tis so:\\nNow I have done. Come, Cominius, pray: look you,\\nSo pressly took, let's see: is dark not of reason;\\nAlack, for our own foul wreck,--which is ildeed,\\nIs that your mother run thy loving,\\nCan you sad well met-horse in your robes.\\nThy face is banish'd, were it from she\\nsick his villow of the king's night that will\"\n",
            " b\"ROMEO:\\nYou'll to the earth to little home;\\nAnd leave the other in Hastings, though\\nLest help the parler to the subject.\\n\\nKING EDWARD IV:\\nWhy that's my hand, and other lives of late,\\nThough suffer us to her love's keavies that looks upon\\nThe pattern of his sword at his mother\\nit is the malaperfel from his proper man.\\nSext lords and honour must not ballad,\\nSave bidst our Romans:\\nThe rage devise for me along.\\n\\nDUKE VINCENTIO:\\nIs despented with him? Go, his, my lodgs,\\nYou that was done to him that she would have.\\n\\nROMEO:\\nTut, that's not in the close with my hand!\\n\\nHENRY BOLINGBROKE:\\nIn jointuil divine, will over my see\\nThat thought there this all-a place,\\nWhere shall your good worship for your highness' hand;\\nAnd thus I took the lady with him.\\nSound-whither fortune will then will have his world\\nTo all that once both wind that blage\\nAs bring thee upon this father, Edward\\nSo stands that slaughter'd him: where the reason\\nEssembal walladiar arms: and, if you live,\\nSo lie that Romeo shall be her wits\"\n",
            " b\"ROMEO:\\nThou art deceived: what may Italk,\\nThat would not living this purpose.\\n\\nSEBASTIAN:\\nLook, how she was\\nAs followers like the name of meat, how beauty,\\nprothees Some harmony in this need;\\nThe day to cloake our hopes on her.\\n\\nCORIOLANUS:\\nHa!\\n\\nCAMILLO:\\nSwear night\\nI know what sort was buznd for you;\\nFor now the dump sidves for ever.\\n\\nSecond Huntings: till now his viper\\nham it, his own:\\n'Tis full throw'd our first Kater.\\n\\nCLARENCE:\\nUntin our boys, which have done we'll swear\\nAs I challen ensure, for no bloody\\nWere all forsoward.\\n\\nVINCENTIO:\\nTurn, good; morrow. O, now, my lord.\\n\\nKING RICHARD III:\\nWhy, strike up!\\n\\nBALTHASAR:\\nI know no less:\\nI should have made deadly were better\\nThat Angelo hath tempted, elsew is distress.\\nBut what a channel's voice?\\n\\nServant:\\nAway, behold, she! for your bloods rage with\\nthee;\\nWhere thou art not confined to all the poor circlude.\\n\\nNORTHUMBERLAND:\\nFear not thyself but temperately; but learn'd his will?\\nWho is it, Lady, and my lord and leave us to our further:\\nI\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.2807905673980713\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ekspor Model Generator"
      ],
      "metadata": {
        "id": "ghj7exifWfqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxFvbYtAWiKd",
        "outputId": "ac7d4e27-ce59-4260-9a26-8c6af9884791"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7c7de020eb30>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3A4aIhZWjs0",
        "outputId": "26dedfc3-cb13-4248-c919-2e8e8d63cc17"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "The ratemble and the knees of the house. Go, jough, so are thou, good die:\n",
            "I have no ngign of beats\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tugas"
      ],
      "metadata": {
        "id": "aP6bIQ19jgJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "    inputs, labels = inputs\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = self(inputs, training=True)\n",
        "      loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    return {'loss': loss}"
      ],
      "metadata": {
        "id": "CpzlSWFijOhR"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "rrsK-jVljlCo"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "metadata": {
        "id": "AiYztt-Tjqk5"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrzVoKwyjuwn",
        "outputId": "b7761794-acf9-488e-e973-5318db7835ca"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 124s 703ms/step - loss: 2.7471\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c7c10705270>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  mean.reset_states()\n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "    logs = model.train_step([inp, target])\n",
        "    mean.update_state(logs['loss'])\n",
        "    if batch_n % 50 == 0:\n",
        "      template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "      print(template)\n",
        "\n",
        "        # saving (checkpoint) the model every 5 epochs\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1AvV0W1kqyk",
        "outputId": "401056dd-c65c-4dee-d6a6-8572f8dfd788"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1984\n",
            "Epoch 1 Batch 50 Loss 2.1015\n",
            "Epoch 1 Batch 100 Loss 1.9839\n",
            "Epoch 1 Batch 150 Loss 1.8940\n",
            "Epoch 2 Batch 0 Loss 1.8384\n",
            "Epoch 2 Batch 50 Loss 1.7903\n",
            "Epoch 2 Batch 100 Loss 1.7129\n",
            "Epoch 2 Batch 150 Loss 1.6467\n",
            "Epoch 3 Batch 0 Loss 1.6081\n",
            "Epoch 3 Batch 50 Loss 1.5867\n",
            "Epoch 3 Batch 100 Loss 1.5076\n",
            "Epoch 3 Batch 150 Loss 1.5083\n",
            "Epoch 4 Batch 0 Loss 1.4027\n",
            "Epoch 4 Batch 50 Loss 1.4794\n",
            "Epoch 4 Batch 100 Loss 1.4540\n",
            "Epoch 4 Batch 150 Loss 1.4367\n",
            "Epoch 5 Batch 0 Loss 1.4599\n",
            "Epoch 5 Batch 50 Loss 1.4112\n",
            "Epoch 5 Batch 100 Loss 1.3956\n",
            "Epoch 5 Batch 150 Loss 1.3781\n",
            "\n",
            "Epoch 5 Loss: 1.3853\n",
            "Time taken for 1 epoch 116.52 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.2751\n",
            "Epoch 6 Batch 50 Loss 1.3263\n",
            "Epoch 6 Batch 100 Loss 1.3208\n",
            "Epoch 6 Batch 150 Loss 1.3314\n",
            "Epoch 7 Batch 0 Loss 1.2078\n",
            "Epoch 7 Batch 50 Loss 1.2861\n",
            "Epoch 7 Batch 100 Loss 1.2653\n",
            "Epoch 7 Batch 150 Loss 1.2686\n",
            "Epoch 8 Batch 0 Loss 1.1958\n",
            "Epoch 8 Batch 50 Loss 1.2559\n",
            "Epoch 8 Batch 100 Loss 1.2586\n",
            "Epoch 8 Batch 150 Loss 1.3033\n",
            "Epoch 9 Batch 0 Loss 1.2005\n",
            "Epoch 9 Batch 50 Loss 1.2180\n",
            "Epoch 9 Batch 100 Loss 1.1908\n",
            "Epoch 9 Batch 150 Loss 1.2107\n",
            "Epoch 10 Batch 0 Loss 1.1252\n",
            "Epoch 10 Batch 50 Loss 1.1588\n",
            "Epoch 10 Batch 100 Loss 1.1505\n",
            "Epoch 10 Batch 150 Loss 1.1849\n",
            "\n",
            "Epoch 10 Loss: 1.1663\n",
            "Time taken for 1 epoch 115.49 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Soal\n",
        "sebutkan perbedaanya dengan praktikum 2?  \n"
      ],
      "metadata": {
        "id": "_y0iu7DEqxLe"
      }
    }
  ]
}